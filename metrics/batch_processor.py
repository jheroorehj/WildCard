"""
Golden Dataset ë°°ì¹˜ ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸

golden_dataset.jsonì˜ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ë¶„ì„í•˜ê³  
ê²°ê³¼ë¥¼ metrics/results/ ì— ì €ì¥í•©ë‹ˆë‹¤.
"""

import asyncio
import json
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from metrics.golden_generator import load_golden_dataset
from metrics.evaluator import BatchEvaluator, MetricsEvaluator
from metrics.storage import save_metrics_json, append_metrics_csv
from core.llm import get_solar_chat


async def create_mock_analysis(input_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    í”„ë¡ íŠ¸ì—”ë“œ ì…ë ¥ì„ ì›Œí¬í”Œë¡œìš° ìƒíƒœë¡œ ë³€í™˜
    (ì‹¤ì œ ë¶„ì„ í•¨ìˆ˜ í˜¸ì¶œ ë˜ëŠ” ëª©ì—… ë°ì´í„°)
    """
    # TODO: ì‹¤ì œ ì›Œí¬í”Œë¡œìš° ì‚¬ìš©
    # from workflow.graph import build_graph
    # graph = build_graph()
    # return await asyncio.to_thread(graph.invoke, state)
    
    # ì„ì‹œ ëª©ì—… ë°ì´í„°
    return {
        "request_id": f"batch_{input_data['layer1_stock']}_{datetime.now().isoformat()}",
        "layer1_stock": input_data["layer1_stock"],
        "layer2_buy_date": input_data["layer2_buy_date"],
        "layer2_sell_date": input_data["layer2_sell_date"],
        "position_status": input_data.get("position_status", "sold"),
        "layer3_decision_basis": input_data["layer3_decision_basis"],
        "n8_loss_cause_analysis": {
            "root_causes": [
                {
                    "title": "Market Condition",
                    "description": "ê¸€ë¡œë²Œ ì‹œì¥ ë³€ë™ì„± ì¦ê°€",
                    "confidence": 0.85
                }
            ]
        },
        "n7_news_analysis": {
            "news_context": {
                "ticker": input_data["layer1_stock"],
                "period": {
                    "buy_date": input_data["layer2_buy_date"],
                    "sell_date": input_data["layer2_sell_date"]
                },
                "key_headlines": [
                    {
                        "title": "Sample News",
                        "content": "ë¶„ì„ ëŒ€ìƒ ë‰´ìŠ¤",
                        "date": input_data["layer2_buy_date"]
                    }
                ]
            }
        },
        "learning_pattern_analysis": {
            "learning_recommendation": {
                "focus_area": "ì‹œì¥ ë¶„ì„",
                "learning_steps": ["Step 1", "Step 2"],
                "recommended_topics": ["Market Analysis", "Risk Management"]
            }
        }
    }


async def process_single_case(
    case: Dict[str, Any],
    evaluator: MetricsEvaluator,
    case_number: int,
    total_cases: int
) -> Dict[str, Any]:
    """
    ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì²˜ë¦¬
    """
    print(f"\n[{case_number}/{total_cases}] {case['id']} - {case['scenario']} ì²˜ë¦¬ ì¤‘...")
    
    try:
        # ë¶„ì„ ì‹¤í–‰
        start_time = datetime.now()
        analysis_result = await create_mock_analysis(case["input"])
        end_time = datetime.now()
        
        # ë©”íŠ¸ë¦­ í‰ê°€
        report = await evaluator.evaluate_all(
            request_id=case["id"],
            start_time=start_time,
            end_time=end_time,
            validation_results=[True, True, True],  # ì„±ê³µ ê°€ì •
            news_data={
                "ticker": case["input"]["layer1_stock"],
                "buy_date": case["input"]["layer2_buy_date"],
                "sell_date": case["input"]["layer2_sell_date"],
                "items": [],
                "dates": []
            },
            analysis_result=analysis_result,
            golden_truth=case,
            save_results=True  # ìë™ ì €ì¥
        )
        
        print(f"âœ“ {case['id']} ì™„ë£Œ")
        return {
            "status": "success",
            "case_id": case["id"],
            "scenario": case["scenario"],
            "metrics_summary": report.get("summary", {})
        }
        
    except Exception as e:
        print(f"âœ— {case['id']} ì‹¤íŒ¨: {str(e)}")
        return {
            "status": "failed",
            "case_id": case["id"],
            "scenario": case["scenario"],
            "error": str(e)
        }


async def run_batch_processing(use_llm: bool = False):
    """
    Golden Dataset ì „ì²´ ë°°ì¹˜ ì²˜ë¦¬
    
    Args:
        use_llm: LLM ê¸°ë°˜ í‰ê°€ í¬í•¨ ì—¬ë¶€
    """
    print("=" * 60)
    print("ğŸš€ Golden Dataset ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘")
    print("=" * 60)
    
    # 1. Golden Dataset ë¡œë“œ
    print("\nğŸ“‚ Golden Dataset ë¡œë“œ ì¤‘...")
    golden_dataset = load_golden_dataset()
    
    if not golden_dataset or not golden_dataset.get("test_cases"):
        print("âŒ Golden Datasetì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    test_cases = golden_dataset["test_cases"]
    print(f"âœ“ {len(test_cases)}ê°œì˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¡œë“œë¨")
    
    # 2. LLM ì´ˆê¸°í™” (í•„ìš” ì‹œ)
    llm = None
    if use_llm:
        print("\nğŸ¤– LLM ì´ˆê¸°í™” ì¤‘...")
        try:
            llm = await get_solar_chat()
            print("âœ“ LLM ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("   ê¸°ë³¸ ë©”íŠ¸ë¦­ë§Œ í‰ê°€í•©ë‹ˆë‹¤.")
    
    # 3. í‰ê°€ê¸° ìƒì„±
    evaluator = MetricsEvaluator(llm=llm)
    
    # 4. ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘
    print(f"\nğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘ ({len(test_cases)}ê°œ ì¼€ì´ìŠ¤)...")
    print("-" * 60)
    
    results = []
    success_count = 0
    failed_count = 0
    
    for i, case in enumerate(test_cases, 1):
        result = await process_single_case(case, evaluator, i, len(test_cases))
        results.append(result)
        
        if result["status"] == "success":
            success_count += 1
        else:
            failed_count += 1
    
    # 5. ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“ˆ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ")
    print("=" * 60)
    print(f"âœ“ ì„±ê³µ: {success_count}/{len(test_cases)}")
    print(f"âœ— ì‹¤íŒ¨: {failed_count}/{len(test_cases)}")
    print(f"\nğŸ’¾ ê²°ê³¼ëŠ” metrics/results/ ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
    
    # 6. ìƒì„¸ ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“‹ ê°œë³„ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    
    for result in results:
        status_icon = "âœ“" if result["status"] == "success" else "âœ—"
        scenario = result.get("scenario", "unknown")
        print(f"{status_icon} {result['case_id']} ({scenario})")
        
        if result["status"] == "success":
            summary = result.get("metrics_summary", {})
            if summary:
                print(f"   Impact: {summary.get('impact', 'N/A')}%")
                print(f"   Trust: {summary.get('trust', 'N/A')}%")
                print(f"   Stability: {summary.get('stability', 'N/A')}%")
        else:
            print(f"   Error: {result.get('error', 'Unknown error')}")
    
    # 7. ìµœì¢… í†µê³„
    print("\n" + "=" * 60)
    print("ğŸ“Š ìµœì¢… í†µê³„")
    print("=" * 60)
    
    overall_results = {
        "timestamp": datetime.now().isoformat(),
        "total_cases": len(test_cases),
        "successful": success_count,
        "failed": failed_count,
        "success_rate": round((success_count / len(test_cases) * 100) if test_cases else 0, 1),
        "individual_results": results,
        "results_location": str(Path(__file__).parent / "results")
    }
    
    print(f"ì²˜ë¦¬ ì‹œê°„: {datetime.now().isoformat()}")
    print(f"ì„±ê³µë¥ : {overall_results['success_rate']}%")
    print(f"ì €ì¥ ìœ„ì¹˜: {overall_results['results_location']}")
    
    return overall_results


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Golden Dataset ë°°ì¹˜ ì²˜ë¦¬")
    parser.add_argument(
        "--llm",
        action="store_true",
        help="LLM ê¸°ë°˜ í‰ê°€ í¬í•¨ (ëŠë¦¬ì§€ë§Œ ì •í™•í•¨)"
    )
    
    args = parser.parse_args()
    
    # ë¹„ë™ê¸° ì‹¤í–‰
    asyncio.run(run_batch_processing(use_llm=args.llm))


if __name__ == "__main__":
    main()
